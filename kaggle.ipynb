{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Normalize, ToPILImage\n",
    "from PIL import Image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "class NYUv2Dataset(data.Dataset):\n",
    "    def __init__(self, train=True):\n",
    "\n",
    "        self.root = \"/kaggle/input/nyu-depth-v2/nyu_data\"\n",
    "        self.train = train\n",
    "        \n",
    "\n",
    "        if train:\n",
    "            csv_data_train = read_csv(self.root + \"/data/nyu2_train.csv\", header=None)\n",
    "            self.rgb_paths = csv_data_train[0]\n",
    "        else:\n",
    "            csv_data_test = read_csv(self.root + \"/data/nyu2_test.csv\", header=None)\n",
    "            self.rgb_paths = csv_data_test[0]\n",
    "\n",
    "        self.length = len(self.rgb_paths)\n",
    "        self.transform = Compose(\n",
    "            [\n",
    "                Resize((240, 320)),\n",
    "                ToTensor(),\n",
    "                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.target_transform = Compose(\n",
    "            [\n",
    "                Resize((120, 160)),\n",
    "                ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.root + \"/\" + self.rgb_paths[index]\n",
    "\n",
    "        rgb = Image.open(path)\n",
    "        if self.train:\n",
    "            path = path.replace(\".jpg\", \".png\")\n",
    "        else:\n",
    "            path = path.replace(\"colors\", \"depth\")\n",
    "        \n",
    "        depth = Image.open(path)\n",
    "\n",
    "        return self.transform(rgb), self.target_transform(depth)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Testing\n",
    "    train_dataset = NYUv2Dataset(train=True)\n",
    "    test_dataset = NYUv2Dataset(train=False)\n",
    "\n",
    "    print('train dataset',len(train_dataset))\n",
    "    print('test dataset',len(test_dataset))\n",
    "    \n",
    "    for item in train_dataset[0]:\n",
    "        print(item.size())\n",
    "    for item in test_dataset[0]:\n",
    "        print(item.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "DenseNet161 = torchvision.models.densenet161(pretrained=False)\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "device = (\n",
    "    torch.device('cuda')\n",
    "    if (torch.cuda.is_available())\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(\"Using device:\", device.type.upper())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NYUv2Dataset(\n",
    "    train=True,\n",
    ")\n",
    "test_dataset = NYUv2Dataset(\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "test_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "\n",
    "split = int(np.floor(test_split * dataset_size))\n",
    "\n",
    "if(shuffle_dataset):\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, sampler = train_sampler\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, sampler = test_sampler\n",
    ")\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottleneck layer\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.bnorm = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bnorm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "# deconvolution layer\n",
    "class DeConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.bnorm = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.deconv = nn.ConvTranspose2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bnorm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define model\n",
    "class Main(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.densenet161 = nn.Sequential(*list(DenseNet161.children())[:-1])\n",
    "        # first bottleneck layer\n",
    "        self.btl1 = Bottleneck(\n",
    "            in_channels=2208, out_channels=512, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "\n",
    "        # 1st deconvolution layer\n",
    "        self.deconv1 = DeConv(\n",
    "            in_channels=512, out_channels=512, kernel_size=5, stride=2, padding=1\n",
    "        )\n",
    "        self.avgpool1 = nn.AvgPool2d(kernel_size=(1, 2), stride=1, padding=0)\n",
    "\n",
    "        # 2nd deconvolution layer\n",
    "        self.deconv2 = DeConv(\n",
    "            in_channels=512, out_channels=256, kernel_size=5, stride=2, padding=1\n",
    "        )\n",
    "        self.avgpool2 = nn.AvgPool2d(kernel_size=(2, 2), stride=1, padding=0)\n",
    "\n",
    "        # 3rd deconvolution layer\n",
    "        self.deconv3 = DeConv(\n",
    "            in_channels=256, out_channels=128, kernel_size=5, stride=2, padding=1\n",
    "        )\n",
    "        self.avgpool3 = nn.AvgPool2d(kernel_size=(2, 2), stride=1, padding=0)\n",
    "\n",
    "        # 4th deconvolution layer\n",
    "        self.deconv4 = DeConv(\n",
    "            in_channels=128, out_channels=1, kernel_size=5, stride=2, padding=1\n",
    "        )\n",
    "        self.avgpool4 = nn.AvgPool2d(kernel_size=(2, 2), stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet161(x)\n",
    "        # print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ after densenet161\", x.size())\n",
    "\n",
    "        x = self.btl1(x)\n",
    "        # print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ after btl1\", x.size())\n",
    "\n",
    "        x = self.deconv1(x)\n",
    "        # print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ after deconv1\", x.size())\n",
    "\n",
    "        x = self.avgpool1(x)\n",
    "        # print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ after avgpool1\", x.size())\n",
    "\n",
    "        x = self.deconv2(x)\n",
    "        # print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ after deconv2\", x.size())\n",
    "\n",
    "        x = self.avgpool2(x)\n",
    "        # print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ after avgpool2\", x.size())\n",
    "\n",
    "        x = self.deconv3(x)\n",
    "        # print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ after deconv3\", x.size())\n",
    "\n",
    "        x = self.avgpool3(x)\n",
    "        # print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ after avgpool3\", x.size())\n",
    "\n",
    "        x = self.deconv4(x)\n",
    "        # print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ after deconv4\", x.size())\n",
    "\n",
    "        x = self.avgpool4(x)\n",
    "        # print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ after avgpool4\", x.size())\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_loss_1 = []\n",
    "all_val_loss_1 = []\n",
    "all_val_accuracy_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Main().to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################### TRAINING AND TESTINGA ###################\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}\\n-------------------------------\")\n",
    "    training_loss_1 = 0\n",
    "    val_loss_1 = 0\n",
    "    val_accuracy_1 = 0\n",
    "    for batch_no,(images_1, labels_1) in enumerate(train_loader):\n",
    "#     for i in range(10):\n",
    "#     if False:\n",
    "        if torch.cuda.is_available():\n",
    "            images, labels = images_1.cuda(), labels_1.cuda()\n",
    "        images = images.float()\n",
    "        optimizer.zero_grad()\n",
    "        output_1 = model(images)\n",
    "        loss_1 = loss_fn(output_1, labels)\n",
    "        loss_1.backward()\n",
    "        training_loss_1 += loss_1.item()\n",
    "        optimizer.step()\n",
    "        if batch_no % 100 == 0:\n",
    "            print(f\"-> TRAIN batch_no: {batch_no}, batch_loss: {loss_1.item()}\")\n",
    "        \n",
    "    else:\n",
    "        # entering evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_images_1, val_labels_1 in test_loader:\n",
    "               \n",
    "                if torch.cuda.is_available():\n",
    "                    val_images, val_labels = val_images_1.cuda(), val_labels_1.cuda()\n",
    "                \n",
    "                val_images = val_images.float()\n",
    "                log_ps_1 = model(val_images)\n",
    "           \n",
    "        \n",
    "                batch_val_loss_1 = loss_fn(log_ps_1, val_labels)\n",
    "                val_loss_1 += batch_val_loss_1.item()\n",
    "\n",
    "                \n",
    "                ps_1 = torch.exp(log_ps_1)\n",
    "                top_p_1, top_class_1 = ps_1.topk(k=1, dim=1)\n",
    "                accuracy_array_1 = (top_class_1 == val_labels.view(*top_class_1.shape))\n",
    "                accuracy_1 = torch.mean(accuracy_array_1.type(torch.FloatTensor))\n",
    "                val_accuracy_1 += accuracy_1.item()\n",
    "\n",
    "\n",
    "                   \n",
    "        \n",
    "        log = f\"\\nTraining loss: {training_loss_1/len(train_loader):.4f}\\t Test loss {val_loss_1/len(test_loader):.4f}\\t\"\n",
    "        log += f\"\\tTest accuracy: {val_accuracy_1/len(test_loader):.4f}\"\n",
    "        \n",
    "        print(log)\n",
    "  \n",
    "        all_training_loss_1.append(training_loss_1/len(train_loader))\n",
    "        all_val_loss_1.append(val_loss_1/len(test_loader))\n",
    "        all_val_accuracy_1.append(val_accuracy_1/len(test_loader))\n",
    "\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "torch.save(model.state_dict(),\"model.pth\")\n",
    "print(\"Saved this model state model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Main()\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt \n",
    "\n",
    "# plt.plot(all_training_loss_1, label=\"training loss\") \n",
    "# plt.plot(all_val_loss_1, label=\"validation loss\") \n",
    "# #plt.plot(all_test_accuracy, label=\"testing accuracy\") plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "model.eval()\n",
    "idx = 2342\n",
    "x, y = train_dataset[idx][0], train_dataset[idx][1]\n",
    "    \n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_prev = x\n",
    "    x = x.unsqueeze(0)\n",
    "    pred = model(x)\n",
    "    pred = pred.squeeze(0)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    rows = 2\n",
    "    columns = 2\n",
    "    \n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow(x_prev.permute(1, 2, 0))\n",
    "    \n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "    plt.imshow(pred.permute(1, 2, 0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
